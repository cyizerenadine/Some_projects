{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Optimizing the descent direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $C:R^d \\rightarrow R$ be a differentiable function and $x_0 \\in R^d$. We are required to solve the following optimisation problem.     $$\\begin{aligned}\n",
    "\\min_{y \\in R^d, ||y|| = 1} \\quad & y^T \\nabla C(x_0)\\\\\n",
    "\\end{aligned}$$ which can generally be written as     $$\\begin{aligned}\n",
    "\\min_{y} \\quad & y^T \\nabla C(x_0)\\\\\n",
    "\\textrm{s.t.} \\quad & ||y|| = 1\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "To solve an optimisation problem, we can use different methods. I will be using Lagrange which have two variables: $y, \\lambda$ where $\\lambda \\in R, y \\in R^d$. So we can define the lagrange function as follows: $L(y,\\lambda) = y^T \\nabla c(x_0) + \\lambda (||y||-1)$ \n",
    "\n",
    "Then we take the derivatoves with respect to the two independent variables:\n",
    "    $$\\frac{\\partial L(y,\\lambda)}{\\partial y} = \\nabla_y L(y,\\lambda) = \\nabla_y C(x_0) + \\lambda \\frac{\\partial ||y||}{\\partial y}$$\n",
    "    $$=\\nabla_y + \\lambda \\frac{y}{||y||}$$\n",
    "    $$\\frac{\\partial L(y,\\lambda)}{\\partial \\lambda} = ||y|| - 1$$\n",
    "Then we solve the partial derivatives as follows to get $\\lambda, y$:\n",
    "    $$\\frac{\\partial L(y,\\lambda)}{\\partial y} = \\frac{\\partial L(y,\\lambda)}{\\partial \\lambda} = 0$$\n",
    "    From the second euqation we can get that $$||y|| = 1$$ \n",
    "    We the substitute this value in the first equation we get:\n",
    "    $$\\nabla_y + \\lambda \\frac{y}{||y||} = \\nabla_yC(x_0) + \\lambda \\frac{y}{1} = 0$$\n",
    "    From this equation we can get the value of $y$ in terms of $\\lambda$ as follows:\n",
    "    $$y* = -\\frac{1}{\\lambda} \\nabla_yC(x_0)$$\n",
    "    putting this value in the equation $||y|| = 1$ we get\n",
    "    $$||-\\frac{1}{\\lambda} \\nabla_yC(x_0)|| = 1$$\n",
    "    Since $\\lambda$ is a constant we can put it outside and $||-x|| = ||x||$ so the value of lambda is \n",
    "    $$ \\lambda = ||\\nabla_yC(x_0)||$$\n",
    "    Putting this value in the value of y we get:\n",
    "    $$y* = -\\frac{ \\nabla_yC(x_0)}{||\\nabla_yC(x_0)||}$$\n",
    "    \n",
    "We can say that the optimal direction from $x_0$ to optimize the $C$ is in the negative direction. so the magnitude of gradient of cost function at $x_0$ gives us the rate of change of $C$ of the steepest descent at $x_0$, so we normalize to obtain a unit vactorthat is in the opposite direction. So the optimal solution $y*$ tells us the direction we should move frm $x_0$ to optimize(minimize) the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: The name of softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $z \\in R^d$ we want to find $$\\lim_{C\\to+\\infty} softmax(Cz)$$.\n",
    "\n",
    "We start by defining the softmax function which can be written as:\n",
    "    $$softmax(z) = \\frac{e^{z_i}}{\\sum_{j=1}^{d} e^{z_j}}$$\n",
    "    so then scaling the softmax by a scale $C$ we get \n",
    "    $$softmax(Cz) = \\frac{e^{Cz_i}}{\\sum_{j=1}^{d} e^{Cz_j}}$$\n",
    "    \n",
    "We can then evaluate the limit as follows:\n",
    "    $$\\lim_{C\\to+\\infty} softmax(Cz) = \\lim_{C\\to+\\infty} \\frac{e^{Cz_i}}{\\sum_{j=1}^{d} e^{Cz_j}}$$\n",
    "    $$= \\frac{\\infty}{\\infty} I.C$$ Which can be solved by L'hopital's rule which is taking derivative on numerator and denominator w.r.t $C$\n",
    "    $$\\lim_{C\\to+\\infty} softmax(Cz) = \\lim_{C\\to+\\infty} \\frac{z_i e^{Cz_i}}{\\sum_{j=1}^{d} z_j e^{Cz_j}}$$\n",
    "    $$= \\lim_{C\\to+\\infty} \\frac{z_i e^{Cz_i}}{z_1e^{Cz_1} + z_2e^{Cz_2} +.....+z_1e^{Cz_1}} $$\n",
    "    So we will have two case: \n",
    "    Let $L$ denote the number of $z_j$ which are equal to the maximum($z_{max}$)\n",
    "    __Case 1: z_i = L__ \n",
    "    $$= \\lim_{C\\to+\\infty} \\frac{z_i e^{Cz_i}}{z_1e^{Cz_1} + z_2e^{Cz_2} +.....+z_1e^{Cz_1}} = \\frac{e^{z_{max}}}{Le^{z_{max}}} = \\frac{1}{L}$$\n",
    "    __Case 2: z_i not equal to  z_{max}__\n",
    "    $$= \\lim_{C\\to+\\infty} \\frac{z_i e^{Cz_i}}{z_1e^{Cz_1} + z_2e^{Cz_2} +.....+z_1e^{Cz_1}} = \\frac{e^{z_i}}{e^{z_{max}}} = \\frac{1}{\\infty} = 0$$\n",
    "    \n",
    "The limit of softmax(Cz) as C approaches infinity is zero if the maximum component of z is non-positive, and infinity if the maximum component of z is positive. When C is large, softmax approaches a maximum function that picks the largest component of z, but when C is small, softmax distributes the weight more evenly among the components of z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: ReLU of a gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G ∼ N (0, \\sigma^2)$ be a gaussian random variable. Let $Y = ReLU(G)$. We need to find $E[Y], Var[Y]$.\n",
    "\n",
    "**Given**:\n",
    "\n",
    "-   ReLU function : $\\text{ReLU}(x) = \\max(0,x)$\n",
    "\n",
    "-   if G is a Gaussian random variable with mean 0 and variance $\\sigma^2$. Then\n",
    "the probability density function of G is given by: $f_G(g) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{g^2}{2\\sigma^2}}$\n",
    "\n",
    "-   $c>0$\n",
    "\n",
    "We calculate the expectation E\\[Y\\] using ReLU and probability density function as follows:\n",
    "$$E[Y] = E[\\text{ReLU}(G)] = \\int_{-\\infty}^{\\infty} \\text{ReLU}(g) f_G(g) dg$$\n",
    "But ReLU function is always positive, so we get :\n",
    "$$E[Y] = \\int_{0}^{\\infty} \\text{ReLU}(g) f_G(g) dg$$\n",
    "For g \\> 0, we have:\n",
    "\n",
    "$$\\text{ReLU}(g) = g$$\n",
    "\n",
    "so we can rewrite the integral as:\n",
    "\n",
    "$$E[Y] = \\int_{0}^{\\infty} g f_G(g) dg$$\n",
    "\n",
    "Using the formula for the probability density function of G, we get:\n",
    "\n",
    "$$E[Y] = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{0}^{\\infty} g e^{-\\frac{g^2}{2\\sigma^2}} dg$$\n",
    "\n",
    "Making the substitution $u = \\frac{g^2}{2\\sigma^2}$, we get:\n",
    "\n",
    "$$E[Y] = \\frac{\\sigma}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} e^{-u} du = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n",
    "\n",
    "Then we find the variance Var\\[Y\\], uisng the definition of  Var\\[Y\\] = E\\[Y\\^2\\] -\n",
    "(E\\[Y\\])\\^2. Since we already know the value of (E\\[Y\\])\\^2 lets calculate\n",
    "E\\[Y\\^2\\]:\n",
    "\n",
    "$$E[Y^2] = E[Y^2] = \\int_{-\\infty}^{\\infty} Y^2 f_G(g) dg$$\n",
    "\n",
    "Since the ReLU function is zero for negative values of g, we have:\n",
    "\n",
    "$$E[Y^2] = \\int_{0}^{\\infty} g^2 f_G(g) dg$$\n",
    "\n",
    "Using the formula for the probability density function of G, we get:\n",
    "\n",
    "$$E[Y^2] = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{0}^{\\infty} g^2 e^{-\\frac{g^2}{2\\sigma^2}} dg$$\n",
    "\n",
    "Making the substitution $u = \\frac{g^2}{2\\sigma^2}$, we get:\n",
    "\n",
    "$$E[Y^2] = \\frac{\\sigma^2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} u e^{-u} du = \\frac{\\sigma^2}{2}$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\\text{Var}[Y] = E[Y^2] - (E[Y])^2 = \\frac{\\sigma^2}{2} - \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right)^2 = \\frac{\\sigma^2}{2} - \\frac{\\sigma^2}{2\\pi}$$\n",
    "\n",
    "Simplifying, we get:\n",
    "$$\\text{Var}[Y] = \\frac{\\sigma^2}{2}\\left(1 - \\frac{1}{\\pi}\\right)$$\n",
    "\n",
    "So the expected value of the ReLU of a Gaussian random variable with mean 0 and variance σ^2 is given by:\n",
    "\n",
    "$$E[\\text{ReLU}(G)] = \\frac{\\sigma}{\\sqrt{2\\pi}}$$\n",
    "\n",
    "And the variance is given by:\n",
    "\n",
    "$$\\text{Var}[\\text{ReLU}(G)] = \\frac{\\sigma^2}{2}\\left(1 - \\frac{1}{\\pi}\\right)$$\n",
    "\n",
    "where G is a Gaussian random variable with mean 0 and variance σ^2, and ReLU is the Rectified Linear Unit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Power of linear neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $n \\leq 3$ be an odd number. define $MAJ_n: \\{-1,1\\}^n \\rightarrow \\{-1,1\\}$ as $$MAJ_n(x_1,...,x_n) = sgn \\left (\\sum_{i=1}^n x_i\\right )$$.Considering a fully connected neural networks with one hidden layer, n inputs neurons and an output neuron. with an activation function given by $$\\sigma(x) = x$$. __Proof of the existance of such neuron__\n",
    "\n",
    "Given $b\\in R^n, w \\in R^{m\\times n}, v\\in R^{1\\times m}, b' \\in R$. define the weighted input into the first hidden layer as follows:\n",
    "$$z_j = \\sum_{i=1}^n w_{ij}x_i + b_j$$ where $1<j<m$ and $1<i<n$. \n",
    "\n",
    "Then we apply the activation function to get $$a_j = \\sigma(z_j) = z_j$$\n",
    "\n",
    "Also write the weighted input to the output layer is $$y = \\sum_{j=1}^m v_j \\left (\\sum_{i=1}^n w_{ij}x_i + b_j\\right) + b'$$\n",
    "also written as:\n",
    "$$y = \\sum_{j=1}^m \\sum_{i=1}^n v_jw_{ij}x_i + \\sum_{i=1}^m b_j + b'$$\n",
    "\n",
    "**Case 1: when $x_i = 1$**\n",
    "$$y = \\sum_{j=1}^m \\sum_{i=1}^n v_jw_{ij} + \\sum_{i=1}^m b_j + b' = 1$$\n",
    "**Case 2: when $x_i = -1$**\n",
    "$$y = -\\sum_{j=1}^m \\sum_{i=1}^n v_jw_{ij} + \\sum_{i=1}^m b_j + b' = -1$$\n",
    "\n",
    "Adding the above equations we get:\n",
    "$$\\sum_{j=1}^m v_jb_j + b' = 0$$\n",
    "\n",
    "Then the output in the output layer will be $$y = \\sum_{j=1}^m \\sum_{i=1}^n v_jw_{ij}x_i$$\n",
    "\n",
    "Consider the case where $\\exists d \\in 1:n$ where $x_d = 1$ and other $x_i = -1$ so we define our output as :\n",
    "$$\\sum_{i=1}^n v_d w_{di} - \\sum_{d\\neq j}^m \\sum_{i=1}^n v_jw_{ij} = -1$$ Then we rewrite the value of the output as\n",
    "$$y = \\sum_{i=1}^n v_d w_{di} + \\sum_{d\\neq j}^m \\sum_{i=1}^n v_jw_{ij}x_i = 1$$\n",
    "Adding the two output functions we get:\n",
    "$$v_d\\sum_{i=1}^n w_{di} = 0$$\n",
    "\n",
    "since weights are generated randomly then we can generalise the above formular as follows:\n",
    "$$v_d\\sum_{j=1}^n w_{ji} = 0, \\forall j$$\n",
    "\n",
    "So from the equation above we get a contradiction that such network can never exist. Hence proved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Early stopping and variable learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements early stopping as a form of regularization to prevent overfitting during training. The basic idea is to monitor the performance of the network on a separate validation set during training, and stop training early if the validation accuracy does not improve for a certain number of epochs (determined by the early_stopping parameter).\n",
    "\n",
    "If the validation accuracy does not improve for early_stopping epochs, training is stopped and the network parameters at the best validation accuracy are returned. This helps to prevent overfitting to the training set, since the network is trained only until it performs well on the validation set.\n",
    "\n",
    "The effect of early stopping is that it can improve the generalization performance of the network by preventing overfitting. However, it can also result in underfitting if the network is stopped too early, i.e., before it has converged to a good solution. Therefore, it is important to set the early_stopping parameter appropriately and monitor the training and validation performance carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_LEN = 28\n",
    "IMG_SIZE = IMG_LEN**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticLoss(object):\n",
    "    @staticmethod\n",
    "    def loss(a, y):\n",
    "        return 0.5*np.dot(a-y, a-y)\n",
    "    @staticmethod\n",
    "    def loss_derivative(a, y):\n",
    "        return a-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    @staticmethod\n",
    "    def loss(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    @staticmethod\n",
    "    def loss_derivative(a, y):\n",
    "        return (a-y)/(a*(1.0-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, n, *, loss=QuadraticLoss, init='standard'):\n",
    "        # Initialize the weights randomly\n",
    "        if init == 'standard':\n",
    "            self.W = np.random.randn(n, IMG_SIZE)\n",
    "            self.V = np.random.randn(10, n)\n",
    "        elif init == 'normalized':\n",
    "            self.W = np.random.randn(n, IMG_SIZE) / np.sqrt(IMG_SIZE)\n",
    "            self.V = np.random.randn(10, n) / np.sqrt(n)\n",
    "        \n",
    "        self.b = np.random.randn(n)\n",
    "        self.bprime = np.random.randn(10)\n",
    "        self.loss = loss\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        # Return the output of a feedforward pass\n",
    "        a = sigmoid(np.dot(self.W, x)+self.b)\n",
    "        return sigmoid(np.dot(self.V, a)+self.bprime)\n",
    "    \n",
    "    def evaluate(self, data, lmbda=0):\n",
    "        \"\"\" Return (cost, accuracy) on the data\"\"\"\n",
    "        correct_samples = 0\n",
    "        total_cost = 0\n",
    "        n_samples = len(data)\n",
    "        \n",
    "        for x, ylabel in data:\n",
    "            y = self.feedforward(x)\n",
    "            prediction = np.argmax(y)\n",
    "            if prediction == ylabel:\n",
    "                correct_samples += 1\n",
    "            \n",
    "            total_cost += self.loss.loss(y, Network.vec_output[ylabel])\n",
    "        \n",
    "        average_cost = total_cost / n_samples\n",
    "        average_cost += 0.5*lmbda*(\n",
    "            np.linalg.norm(self.W)**2 + np.linalg.norm(self.V)**2)\n",
    "        \n",
    "        return average_cost, correct_samples / n_samples\n",
    "            \n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda):\n",
    "        # Run backprop and update weights on the minibatch\n",
    "        k = len(mini_batch)\n",
    "        delta_W = np.zeros(self.W.shape)\n",
    "        delta_b = np.zeros(self.b.shape)\n",
    "        delta_V = np.zeros(self.V.shape)\n",
    "        delta_bprime = np.zeros(self.bprime.shape)\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            nabla_W, nabla_b, nabla_V, \\\n",
    "                nabla_bprime = self.backprop(x, y)\n",
    "            delta_W += nabla_W\n",
    "            delta_b += nabla_b\n",
    "            delta_V += nabla_V\n",
    "            delta_bprime += nabla_bprime\n",
    "        \n",
    "        self.W -= eta*(lmbda*self.W + 1/k * delta_W)\n",
    "        self.b -= eta/k * delta_b\n",
    "        self.V -= eta*(lmbda*self.V + 1/k * delta_V)\n",
    "        self.bprime -= eta/k * delta_bprime\n",
    "\n",
    "    def SGD(self, training_data, mini_batch_size, eta, test_data = [], lmbda=0\n",
    "            , early_stopping=10, tol = 0.001):\n",
    "        # Initialize variables for early stopping\n",
    "        best_val_acc = 0\n",
    "        epochs_without_improvement = 0\n",
    "        epoch = 0\n",
    "\n",
    "        res = []\n",
    "\n",
    "        while epochs_without_improvement < early_stopping:\n",
    "            # Shuffle the training data\n",
    "            np.random.shuffle(training_data)\n",
    "            for j in range(0, len(training_data), mini_batch_size):\n",
    "                    mini_batch = training_data[j:j+mini_batch_size]\n",
    "                    self.update_mini_batch(mini_batch, eta, lmbda)\n",
    "\n",
    "            train_cost, train_acc = self.evaluate(training_data, lmbda)\n",
    "            test_cost, test_acc = self.evaluate(test_data, lmbda)\n",
    "            res.append((train_cost, train_acc, test_cost, test_acc))\n",
    "\n",
    "\n",
    "            if (train_acc - best_val_acc)< tol:\n",
    "                epochs_without_improvement += 1\n",
    "               \n",
    "            else:\n",
    "                epochs_without_improvement = 0\n",
    "            \n",
    "            best_val_acc = train_acc\n",
    "\n",
    "        return res, epoch\n",
    "        \n",
    "    def backprop(self, x, ylabel):\n",
    "        # feedforward\n",
    "        z1 = np.dot(self.W, x)+self.b\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(self.V, a1)+self.bprime\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        # backward\n",
    "        delta_2 = self.loss.loss_derivative(a2, Network.vec_output[ylabel]) * sigmoid_prime(z2)\n",
    "        nabla_bprime = delta_2\n",
    "        nabla_V = np.outer(delta_2, a1)\n",
    "        \n",
    "        delta_1 = np.dot(self.V.transpose(), delta_2) * sigmoid_prime(z1)\n",
    "        nabla_b = delta_1\n",
    "        nabla_W = np.outer(delta_1, x)\n",
    "        \n",
    "        return nabla_W, nabla_b, nabla_V, nabla_bprime\n",
    "    \n",
    "    vec_output = []\n",
    "    for ylabel in range(10):\n",
    "        V = np.zeros(10)\n",
    "        V[ylabel] = 1\n",
    "        vec_output.append(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1.0+np.exp(-z))\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Note you have to update the path below\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return training_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN, VALIDATION, TEST = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_data(D):\n",
    "    return list(zip(D[0], D[1]))\n",
    "train_data = zip_data(TRAIN)\n",
    "validation_data = zip_data(VALIDATION)\n",
    "test_data = zip_data(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET = Network(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = NET.SGD(train_data,10, 3,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.09169238011967447, 0.88952, 0.09185440094445046, 0.8863),\n",
       "  (0.0638864591854551, 0.91996, 0.06520238404976186, 0.9188),\n",
       "  (0.0561798864758781, 0.93064, 0.05907595413003845, 0.9285),\n",
       "  (0.052902320712857, 0.93628, 0.056537325638313166, 0.9321),\n",
       "  (0.04867597873953349, 0.9416, 0.05381363636003624, 0.9368),\n",
       "  (0.049094472805613916, 0.94198, 0.05406368737985452, 0.9356),\n",
       "  (0.0465253348899741, 0.94622, 0.053933584034757426, 0.9372),\n",
       "  (0.043350474395163895, 0.94842, 0.051197983556948684, 0.9383),\n",
       "  (0.04108669228880043, 0.95166, 0.04990746318036264, 0.9414),\n",
       "  (0.0407960316051081, 0.95114, 0.04980121333458977, 0.9392),\n",
       "  (0.03933960627954261, 0.95338, 0.04804567765103793, 0.9431),\n",
       "  (0.03792046506247093, 0.95534, 0.04759144185699238, 0.9442),\n",
       "  (0.03907503946368302, 0.95422, 0.04932685584275804, 0.9425),\n",
       "  (0.03589865340645504, 0.95796, 0.04583501395851755, 0.9482),\n",
       "  (0.037304657527079815, 0.95788, 0.049243578363090644, 0.9445),\n",
       "  (0.034527210351501976, 0.95984, 0.04717071221832707, 0.9463),\n",
       "  (0.03243160605708209, 0.9625, 0.04713268091860908, 0.9449),\n",
       "  (0.03360802290277487, 0.96206, 0.04683143345187695, 0.9458),\n",
       "  (0.03149280415667971, 0.96328, 0.04564322922786234, 0.947),\n",
       "  (0.031147574010787875, 0.96384, 0.045457061347651304, 0.9476),\n",
       "  (0.03270694566023184, 0.9618, 0.04841408049030369, 0.9436),\n",
       "  (0.03180192211981294, 0.96326, 0.04667931599973966, 0.9452),\n",
       "  (0.029470386015303992, 0.96544, 0.04474617382666885, 0.9495),\n",
       "  (0.031319596156871965, 0.9635, 0.04576373965500789, 0.9492),\n",
       "  (0.033180347356288956, 0.96316, 0.0487480405233814, 0.9458),\n",
       "  (0.02969789940601632, 0.96544, 0.0452582887102109, 0.9477),\n",
       "  (0.027276372569043264, 0.96832, 0.04446510438204783, 0.9496),\n",
       "  (0.026487567680029987, 0.96922, 0.04389734440789531, 0.949),\n",
       "  (0.027975024332774823, 0.96726, 0.04403362121666456, 0.9491),\n",
       "  (0.02923717926744958, 0.9666, 0.045318466120874104, 0.9492),\n",
       "  (0.03031944055119496, 0.96424, 0.04719286085663465, 0.9464),\n",
       "  (0.02525495722014965, 0.971, 0.04504488347682241, 0.9486),\n",
       "  (0.026908425205015, 0.96852, 0.04579752347513306, 0.9482),\n",
       "  (0.025713369929353847, 0.9704, 0.04549552589401985, 0.9488),\n",
       "  (0.02510390181207515, 0.97068, 0.044459871617483375, 0.9505),\n",
       "  (0.025629083843933795, 0.97122, 0.045716679500504405, 0.9484),\n",
       "  (0.02488939075268222, 0.97162, 0.0452020984396971, 0.9481),\n",
       "  (0.024567415210576486, 0.97136, 0.044361112279117286, 0.949),\n",
       "  (0.023782076583966295, 0.9721, 0.044596617474909485, 0.9511),\n",
       "  (0.028706720321598524, 0.96652, 0.04702264913243135, 0.9467),\n",
       "  (0.02226620487013512, 0.97408, 0.04381589813866076, 0.9512),\n",
       "  (0.022530614955400506, 0.97344, 0.04435604239201788, 0.9501),\n",
       "  (0.02435651054549703, 0.97176, 0.0477163996950725, 0.945),\n",
       "  (0.02494227394709629, 0.97214, 0.04480417053038035, 0.9506),\n",
       "  (0.022339998193561233, 0.9736, 0.04373416696323874, 0.9496),\n",
       "  (0.022582805097524535, 0.97364, 0.04506517759191735, 0.9501),\n",
       "  (0.02704870462776476, 0.96996, 0.04768330526865644, 0.949),\n",
       "  (0.02165921674406503, 0.97402, 0.04561297358252854, 0.948),\n",
       "  (0.020939040058654664, 0.9749, 0.04468869219222821, 0.9498),\n",
       "  (0.021990084810998983, 0.97456, 0.04491930563670573, 0.9494),\n",
       "  (0.021750963151976208, 0.9742, 0.0447831013836435, 0.9499),\n",
       "  (0.02044474840887173, 0.97564, 0.043599246459125975, 0.9507),\n",
       "  (0.021223485947129563, 0.97524, 0.04618702600537846, 0.9486),\n",
       "  (0.02079511033475884, 0.97514, 0.044728470337367994, 0.9498),\n",
       "  (0.020685556884353373, 0.97534, 0.04408627838273979, 0.9507),\n",
       "  (0.019885356496608303, 0.97632, 0.044172961326274844, 0.9508),\n",
       "  (0.021166949905672163, 0.97584, 0.04532785112263099, 0.9479),\n",
       "  (0.019101561843675875, 0.97668, 0.04369983513809153, 0.9504),\n",
       "  (0.020307380657338194, 0.97588, 0.04520858365508672, 0.9488),\n",
       "  (0.02099969924060076, 0.97558, 0.04576956285088942, 0.9481),\n",
       "  (0.021015210666989585, 0.97602, 0.045788881484012646, 0.9501),\n",
       "  (0.02163538287939826, 0.97532, 0.04562050206723156, 0.9501)],\n",
       " 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Variable learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above includes a variable learning rate implemented using a simple learning rate schedule. The learning rate eta is divided by 10 after every 3 epochs of training. This means that the learning rate decreases over time as the model approaches convergence, which can help it avoid overshooting the minimum and achieve better performance.\n",
    "\n",
    "As can be seen, the learning rate eta is updated by dividing it by 10 at the start of each loop, and then used in self.update_mini_batch function to update the model weights during training. This process is repeated for a total of three times, each time training the model for a certain number of epochs (until early stopping conditions are met)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, n, *, loss=QuadraticLoss, init='standard'):\n",
    "        # Initialize the weights randomly\n",
    "        if init == 'standard':\n",
    "            self.W = np.random.randn(n, IMG_SIZE)\n",
    "            self.V = np.random.randn(10, n)\n",
    "        elif init == 'normalized':\n",
    "            self.W = np.random.randn(n, IMG_SIZE) / np.sqrt(IMG_SIZE)\n",
    "            self.V = np.random.randn(10, n) / np.sqrt(n)\n",
    "        \n",
    "        self.b = np.random.randn(n)\n",
    "        self.bprime = np.random.randn(10)\n",
    "        self.loss = loss\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        # Return the output of a feedforward pass\n",
    "        a = sigmoid(np.dot(self.W, x)+self.b)\n",
    "        return sigmoid(np.dot(self.V, a)+self.bprime)\n",
    "    \n",
    "    def evaluate(self, data, lmbda=0):\n",
    "        \"\"\" Return (cost, accuracy) on the data\"\"\"\n",
    "        correct_samples = 0\n",
    "        total_cost = 0\n",
    "        n_samples = len(data)\n",
    "        \n",
    "        for x, ylabel in data:\n",
    "            y = self.feedforward(x)\n",
    "            prediction = np.argmax(y)\n",
    "            if prediction == ylabel:\n",
    "                correct_samples += 1\n",
    "            \n",
    "            total_cost += self.loss.loss(y, Network.vec_output[ylabel])\n",
    "        \n",
    "        average_cost = total_cost / n_samples\n",
    "        average_cost += 0.5*lmbda*(\n",
    "            np.linalg.norm(self.W)**2 + np.linalg.norm(self.V)**2)\n",
    "        \n",
    "        return average_cost, correct_samples / n_samples\n",
    "            \n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda):\n",
    "        # Run backprop and update weights on the minibatch\n",
    "        k = len(mini_batch)\n",
    "        delta_W = np.zeros(self.W.shape)\n",
    "        delta_b = np.zeros(self.b.shape)\n",
    "        delta_V = np.zeros(self.V.shape)\n",
    "        delta_bprime = np.zeros(self.bprime.shape)\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            nabla_W, nabla_b, nabla_V, \\\n",
    "                nabla_bprime = self.backprop(x, y)\n",
    "            delta_W += nabla_W\n",
    "            delta_b += nabla_b\n",
    "            delta_V += nabla_V\n",
    "            delta_bprime += nabla_bprime\n",
    "        \n",
    "        self.W -= eta*(lmbda*self.W + 1/k * delta_W)\n",
    "        self.b -= eta/k * delta_b\n",
    "        self.V -= eta*(lmbda*self.V + 1/k * delta_V)\n",
    "        self.bprime -= eta/k * delta_bprime\n",
    "\n",
    "    def SGD(self, training_data, mini_batch_size, eta, test_data = [], lmbda=0\n",
    "            , early_stopping=10, tol = 0.001):\n",
    "        # Initialize variables for early stopping\n",
    "        best_val_acc = 0\n",
    "        epochs_without_improvement = 0\n",
    "        epoch = 0\n",
    "        res = []\n",
    "        for i in range(3):\n",
    "            best_val_acc = 0\n",
    "            epochs_without_improvement = 0\n",
    "            epoch = 0\n",
    "            eta = eta/10\n",
    "            while epochs_without_improvement <= early_stopping:\n",
    "                # Shuffle the training data\n",
    "                np.random.shuffle(training_data)\n",
    "                for j in range(0, len(training_data), mini_batch_size):\n",
    "                        mini_batch = training_data[j:j+mini_batch_size]\n",
    "                        self.update_mini_batch(mini_batch, eta, lmbda)\n",
    "\n",
    "                train_cost, train_acc = self.evaluate(training_data, lmbda)\n",
    "                test_cost, test_acc = self.evaluate(test_data, lmbda)\n",
    "                res.append((train_cost, train_acc, test_cost, test_acc))\n",
    "\n",
    "\n",
    "                if (train_acc - best_val_acc)< tol:\n",
    "                    epochs_without_improvement += 1\n",
    "\n",
    "                else:\n",
    "                    epochs_without_improvement = 0\n",
    "\n",
    "                best_val_acc = train_acc\n",
    "\n",
    "        return res, epoch\n",
    "        \n",
    "    def backprop(self, x, ylabel):\n",
    "        # feedforward\n",
    "        z1 = np.dot(self.W, x)+self.b\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(self.V, a1)+self.bprime\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        # backward\n",
    "        delta_2 = self.loss.loss_derivative(a2, Network.vec_output[ylabel]) * sigmoid_prime(z2)\n",
    "        nabla_bprime = delta_2\n",
    "        nabla_V = np.outer(delta_2, a1)\n",
    "        \n",
    "        delta_1 = np.dot(self.V.transpose(), delta_2) * sigmoid_prime(z1)\n",
    "        nabla_b = delta_1\n",
    "        nabla_W = np.outer(delta_1, x)\n",
    "        \n",
    "        return nabla_W, nabla_b, nabla_V, nabla_bprime\n",
    "    \n",
    "    vec_output = []\n",
    "    for ylabel in range(10):\n",
    "        V = np.zeros(10)\n",
    "        V[ylabel] = 1\n",
    "        vec_output.append(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET2 = Network(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = NET2.SGD(train_data, 10, 3, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.2725090904675947, 0.56362, 0.2707898281723795, 0.5656),\n",
       "  (0.17856628312049294, 0.7499, 0.1768998636702614, 0.7501),\n",
       "  (0.12098326237157477, 0.85156, 0.11618021517890224, 0.8574),\n",
       "  (0.10290921972645803, 0.87204, 0.09944820882600404, 0.8751),\n",
       "  (0.09250627598156576, 0.8862, 0.08977012202652945, 0.8894),\n",
       "  (0.08631643672213937, 0.89282, 0.0847510139013321, 0.8958),\n",
       "  (0.0811523409306929, 0.89936, 0.0805112110674262, 0.9025),\n",
       "  (0.07682442914276061, 0.90406, 0.07647216773098399, 0.9055),\n",
       "  (0.07380867097030848, 0.9074, 0.07426424626344957, 0.9085),\n",
       "  (0.07039379419292442, 0.91246, 0.07153752548963008, 0.9123),\n",
       "  (0.06826952258621896, 0.91478, 0.06942726297440914, 0.9153),\n",
       "  (0.06568953740486791, 0.91846, 0.06774342430940734, 0.9161),\n",
       "  (0.06374649127351287, 0.92096, 0.0658854634119148, 0.9193),\n",
       "  (0.062066632134470556, 0.92314, 0.0649863531306896, 0.92),\n",
       "  (0.060826929951525874, 0.92474, 0.0640168052508601, 0.9198),\n",
       "  (0.05882218128173874, 0.92768, 0.06248056722742417, 0.9232),\n",
       "  (0.057365086524393406, 0.9298, 0.061348739518487946, 0.9237),\n",
       "  (0.05610292200713876, 0.93176, 0.06069577228483317, 0.9244),\n",
       "  (0.055306283689228464, 0.93238, 0.05999059305195517, 0.9259),\n",
       "  (0.05412480331350136, 0.93378, 0.05934995181913795, 0.9253),\n",
       "  (0.05335034398922351, 0.93524, 0.05912743563871101, 0.9263),\n",
       "  (0.05201618576346291, 0.9369, 0.05782164455146727, 0.9293),\n",
       "  (0.05107124541217848, 0.93844, 0.05723205514766774, 0.9306),\n",
       "  (0.05032176156357344, 0.93982, 0.05668067701555121, 0.9323),\n",
       "  (0.049350309644086404, 0.9406, 0.05626241689097382, 0.932),\n",
       "  (0.04894358535983454, 0.94118, 0.05609658831368015, 0.9311),\n",
       "  (0.047808224891070894, 0.94276, 0.05504124455073427, 0.9339),\n",
       "  (0.04742124908991443, 0.94336, 0.05465435447618313, 0.9336),\n",
       "  (0.046327256114048615, 0.94462, 0.05426404996846951, 0.9344),\n",
       "  (0.045826949598817945, 0.9453, 0.05371051558413503, 0.9359),\n",
       "  (0.04513514116757341, 0.94626, 0.05326088724212087, 0.9376),\n",
       "  (0.04466512051790095, 0.94706, 0.053148003062055395, 0.9363),\n",
       "  (0.0438845841367348, 0.94792, 0.05295784645455093, 0.9372),\n",
       "  (0.043332634521391786, 0.94906, 0.05239479113896805, 0.937),\n",
       "  (0.04315619406787082, 0.94896, 0.05253474456408494, 0.9364),\n",
       "  (0.04268768886384874, 0.94974, 0.0524549730851563, 0.937),\n",
       "  (0.04191134925959739, 0.9505, 0.051571223623900514, 0.9384),\n",
       "  (0.04154531327076211, 0.95078, 0.05155160306714143, 0.9384),\n",
       "  (0.041264417834203135, 0.95192, 0.051767953064911845, 0.9387),\n",
       "  (0.04045222329939784, 0.95252, 0.05118790325062168, 0.9381),\n",
       "  (0.04017223427872744, 0.9527, 0.051191270679763264, 0.9373),\n",
       "  (0.03980493385420048, 0.95314, 0.051240408208776636, 0.9382),\n",
       "  (0.03938782881192936, 0.95398, 0.051088704936870934, 0.938),\n",
       "  (0.0389994626025029, 0.95396, 0.0509981431790581, 0.9386),\n",
       "  (0.038571739230270394, 0.95492, 0.05070986944216535, 0.9389),\n",
       "  (0.03857610527520195, 0.95504, 0.050126852597870176, 0.9395),\n",
       "  (0.03764484420894808, 0.95674, 0.050031373412765534, 0.9404),\n",
       "  (0.037572423104207324, 0.95642, 0.050097995066276844, 0.9395),\n",
       "  (0.03690075346780723, 0.9572, 0.04992109182023495, 0.9403),\n",
       "  (0.0367214747113966, 0.9576, 0.04977714421493169, 0.9402),\n",
       "  (0.03626113694866248, 0.95828, 0.04950581225590338, 0.9404),\n",
       "  (0.03592029198698583, 0.95818, 0.049473932511209344, 0.94),\n",
       "  (0.03563745145039948, 0.95884, 0.049373343724986515, 0.9403),\n",
       "  (0.03545492866289239, 0.95894, 0.04920544088023568, 0.9419),\n",
       "  (0.03513558281579173, 0.95918, 0.049324956111841706, 0.9412),\n",
       "  (0.03488040658330457, 0.95956, 0.049065113757228634, 0.9411),\n",
       "  (0.03449021645049401, 0.96016, 0.0491251308475153, 0.9415),\n",
       "  (0.03433522843420969, 0.96006, 0.04902863997258311, 0.9414),\n",
       "  (0.03397260918179733, 0.96048, 0.04882796481608651, 0.9409),\n",
       "  (0.033921639171885606, 0.96068, 0.048788466410631726, 0.9412),\n",
       "  (0.03390347521731089, 0.96072, 0.048817713127540754, 0.9409),\n",
       "  (0.03387263959251881, 0.96062, 0.04878600239888583, 0.9411),\n",
       "  (0.033829217866851224, 0.96078, 0.04874865131470772, 0.9409),\n",
       "  (0.03379375726948344, 0.9608, 0.04875574080852163, 0.9408),\n",
       "  (0.03377135060614663, 0.9609, 0.048738797927798234, 0.9412),\n",
       "  (0.033744047526688775, 0.961, 0.04871194401568316, 0.9412),\n",
       "  (0.03372730793784115, 0.96084, 0.04878450508386742, 0.941),\n",
       "  (0.033696267240199806, 0.9609, 0.048756504584501, 0.941),\n",
       "  (0.03367249527554946, 0.96116, 0.04877751939022536, 0.9411),\n",
       "  (0.03363534527691446, 0.96106, 0.04872327073006155, 0.9413),\n",
       "  (0.03362739322368565, 0.96108, 0.04871410985341579, 0.9413),\n",
       "  (0.03362209868654703, 0.96102, 0.04870685789333579, 0.9412),\n",
       "  (0.03361822446242789, 0.96102, 0.048703092687843154, 0.9413),\n",
       "  (0.033614974408258974, 0.96102, 0.04870184789611509, 0.9413),\n",
       "  (0.03361199581377648, 0.96106, 0.048699909703293506, 0.9413),\n",
       "  (0.0336091339363855, 0.96106, 0.04869917303079165, 0.9412),\n",
       "  (0.03360637145566788, 0.96108, 0.0486974302246395, 0.9412),\n",
       "  (0.033603689722348125, 0.96108, 0.048696591235300735, 0.9412),\n",
       "  (0.03360096214956886, 0.96108, 0.048696090047000685, 0.9412),\n",
       "  (0.03359827630563363, 0.9611, 0.04869516086560095, 0.9412),\n",
       "  (0.033595625948679896, 0.9611, 0.04869395491771243, 0.9412),\n",
       "  (0.03359298811345102, 0.96112, 0.048693376494944554, 0.9413)],\n",
       " 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Implementing convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.0\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the neural network from the lecture slide “MNIST example: architecture 1”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a neural network with a convolutional layer and two fully connected layers. The neural network is used for classification tasks. The convolutional layer performs feature extraction on the input image, followed by max pooling. The fully connected layers then process the features extracted by the convolutional layer to produce a final classification output. The network is trained using backpropagation and stochastic gradient descent. The evaluate function computes the accuracy and cost of the network on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, n):\n",
    "\n",
    "        # Initialize the weights randomly\n",
    "        self.W = np.random.randn(n, 2880)\n",
    "        self.b = np.random.randn(n)\n",
    "        self.V = np.random.randn(10, n)\n",
    "        self.bprime = np.random.randn(10)\n",
    "        \n",
    "    def ConvLayer(self, input_tensor):\n",
    "        \n",
    "        # Define weight tensor with 20 filters\n",
    "        num_filters = 20\n",
    "        filter_height, filter_width = 5, 5\n",
    "\n",
    "        weight_tensor = np.random.randn(num_filters, filter_height, filter_width)\n",
    "\n",
    "        # Define bias tensor for each filter\n",
    "        bias_tensor = np.random.randn(num_filters)\n",
    "\n",
    "        # Define stride and padding\n",
    "        stride = 1\n",
    "        padding = 0\n",
    "        \n",
    "        # Define pooling parameters\n",
    "        pool_size = 2\n",
    "        pool_stride = 2\n",
    "\n",
    "        # Compute output shape\n",
    "        input_height, input_width = input_tensor.shape\n",
    "        conv_output_height = int((input_height - filter_height + 2 * padding) / stride) + 1\n",
    "        conv_output_width = int((input_width - filter_width + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Initialize output tensor\n",
    "        conv_output_tensor = np.zeros((num_filters, conv_output_height, conv_output_width))\n",
    "\n",
    "        # Perform matrix multiplication with weights\n",
    "        for k in range(num_filters):\n",
    "            for i in range(conv_output_height):\n",
    "                for j in range(conv_output_width):\n",
    "                    conv_output_tensor[k][i][j] = np.sum(\n",
    "                        input_tensor[i:i+filter_height, j:j+filter_width] * weight_tensor[k]) \\\n",
    "                    + bias_tensor[k]       \n",
    "        \n",
    "        # Apply ReLU activation function to convolutional output tensor\n",
    "        conv_output_tensor = np.maximum(conv_output_tensor, 0)\n",
    "        \n",
    "        # Compute output shape for max pooling\n",
    "        pool_output_height = int((conv_output_height - pool_size) / pool_stride) + 1\n",
    "        pool_output_width = int((conv_output_width - pool_size) / pool_stride) + 1\n",
    "\n",
    "        # Initialize output tensor for max pooling\n",
    "        pool_output_tensor = np.zeros((num_filters, pool_output_height, pool_output_width))\n",
    "\n",
    "        # Perform max pooling\n",
    "        for k in range(num_filters):\n",
    "            for i in range(pool_output_height):\n",
    "                for j in range(pool_output_width):\n",
    "                    pool_output_tensor[k][i][j] = np.max(\n",
    "                        conv_output_tensor[k][i*pool_stride:i*pool_stride+pool_size, \\\n",
    "                                              j*pool_stride:j*pool_stride+pool_size])\n",
    "        return pool_output_tensor.flatten()\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        # Return the output of a feedforward pass\n",
    "        conv_output = self.ConvLayer(x)\n",
    "        a = np.maximum(np.dot(self.W, conv_output) + self.b, 0)\n",
    "        return sigmoid(np.dot(self.V, a)+self.bprime)\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        \"\"\" Return (cost, accuracy) on the data\"\"\"\n",
    "        correct_samples = 0\n",
    "        total_cost = 0\n",
    "        n_samples = len(data)\n",
    "\n",
    "        for x, ylabel in data:\n",
    "            y = self.feedforward(x)\n",
    "            prediction = np.argmax(y)\n",
    "            if prediction == ylabel:\n",
    "                correct_samples += 1\n",
    "\n",
    "            y[ylabel] -= 1.0\n",
    "            total_cost += np.dot(y, y)\n",
    "\n",
    "        return correct_samples / n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET3 = Network(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_data(D0,D1):\n",
    "    return list(zip(D0, D1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 1.20091504e-40, 3.33466272e-76, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.17239705e-81, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NET3.feedforward(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = zip_data(train_data[:1000], train_labels[:1000])\n",
    "test_set = zip_data(test_data[:100], test_labels[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of 0.094 for NET3.evaluate(train_set) on the MNIST dataset does not make sense. This indicates that the network is performing poorly on the training set, with only 9.4% of the samples being classified correctly. The network needs to be improved by adjusting the hyperparameters, such as the learning rate, number of layers, number of nodes in each layer, or regularization techniques, such as dropout or weight decay. Additionally, it may be necessary to further tune the hyperparameters through experimentation to achieve better performance on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-46a8ef3da670>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1.0+np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.094"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NET3.evaluate(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
